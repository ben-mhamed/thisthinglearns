{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake using Deep Q-Learning\n",
    "<hr>\n",
    "**Author: Sahil Johari**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO:\n",
    "* Change DQNetwork to a minimal level\n",
    "* Parallelize Convolution process and look for other points for the same\n",
    "* Compile the project into a presentable format\n",
    "* Add comments to the code and clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "_Need to write this after the complete implementation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling a Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "import numpy as np\n",
    "\n",
    "class DQNetwork:\n",
    "    def __init__(self, actions, input_shape, alpha=0.1, gamma=0.99,\n",
    "                 dropout_prob=0.1, load_path='', logger=None):\n",
    "        self.model = Sequential()\n",
    "        self.actions = actions  # Size of the network output\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # Define neural network\n",
    "        self.model.add(BatchNormalization(axis=1, input_shape=input_shape))\n",
    "        self.model.add(Convolution2D(32, 2, 2, border_mode='valid',\n",
    "                                     subsample=(2, 2), dim_ordering='th'))\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "        self.model.add(BatchNormalization(axis=1))\n",
    "        self.model.add(Convolution2D(64, 2, 2, border_mode='valid',\n",
    "                                     subsample=(2, 2), dim_ordering='th'))\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "        self.model.add(BatchNormalization(axis=1))\n",
    "        self.model.add(Convolution2D(64, 3, 3, border_mode='valid',\n",
    "                                     subsample=(2, 2), dim_ordering='th'))\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "        self.model.add(Flatten())\n",
    "\n",
    "        self.model.add(Dropout(self.dropout_prob))\n",
    "        self.model.add(Dense(512))\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "        self.model.add(Dense(self.actions))\n",
    "\n",
    "        self.optimizer = Adam()\n",
    "        self.logger = logger\n",
    "\n",
    "        # Load the network from saved model\n",
    "        if load_path != '':\n",
    "            self.load(load_path)\n",
    "\n",
    "        self.model.compile(loss='mean_squared_error', optimizer=self.optimizer,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def train(self, batch):\n",
    "        \"\"\"\n",
    "        Generates inputs and targets from the given batch, trains the model on\n",
    "        them.\n",
    "        :param batch: iterable of dictionaries with keys 'source', 'action',\n",
    "        'dest', 'reward'\n",
    "        \"\"\"\n",
    "        x_train = []\n",
    "        t_train = []\n",
    "\n",
    "        # Generate training set and targets\n",
    "        for datapoint in batch:\n",
    "            x_train.append(datapoint['source'].astype(np.float64))\n",
    "\n",
    "            # Get the current Q-values for the next state and select the best\n",
    "            next_state_pred = self.predict(datapoint['dest'].astype(np.float64)).ravel()\n",
    "            next_q_value = np.max(next_state_pred)\n",
    "\n",
    "            # The error must be 0 on all actions except the one taken\n",
    "            t = list(self.predict(datapoint['source'])[0])\n",
    "            if datapoint['final']:\n",
    "                t[datapoint['action']] = datapoint['reward']\n",
    "            else:\n",
    "                t[datapoint['action']] = datapoint['reward'] + \\\n",
    "                                         self.gamma * next_q_value\n",
    "\n",
    "            t_train.append(t)\n",
    "\n",
    "        # Prepare inputs and targets\n",
    "        x_train = np.asarray(x_train).squeeze()\n",
    "        t_train = np.asarray(t_train).squeeze()\n",
    "\n",
    "        # Train the model for one epoch\n",
    "        h = self.model.fit(x_train,\n",
    "                           t_train,\n",
    "                           batch_size=32,\n",
    "                           nb_epoch=1)\n",
    "\n",
    "        # Log loss and accuracy\n",
    "        if self.logger is not None:\n",
    "            self.logger.to_csv('loss_history.csv',\n",
    "                               [h.history['loss'][0], h.history['acc'][0]])\n",
    "\n",
    "    def predict(self, state):\n",
    "        \"\"\"\n",
    "        Feeds state into the model, returns predicted Q-values.\n",
    "        :param state: a numpy.array with same shape as the network's input\n",
    "        :return: numpy.array with predicted Q-values\n",
    "        \"\"\"\n",
    "        state = state.astype(np.float64)\n",
    "        return self.model.predict(state, batch_size=1)\n",
    "\n",
    "    def save(self, filename=None):\n",
    "        \"\"\"\n",
    "        Saves the model weights to disk.\n",
    "        :param filename: file to which save the weights (must end with \".h5\")\n",
    "        \"\"\"\n",
    "        f = ('model.h5' if filename is None else filename)\n",
    "        if self.logger is not None:\n",
    "            self.logger.log('Saving model as %s' % f)\n",
    "#         self.model.save_weights(self.logger.path + f)\n",
    "        self.model.save_weights(f)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Loads the model's weights from path.\n",
    "        :param path: h5 file from which to load teh weights\n",
    "        \"\"\"\n",
    "        if self.logger is not None:\n",
    "            self.logger.log('Loading weights from file...')\n",
    "        self.model.load_weights(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQAgent:\n",
    "    def __init__(self,\n",
    "                 actions,\n",
    "                 batch_size=1024,\n",
    "                 alpha=0.01,\n",
    "                 gamma=0.9,\n",
    "                 dropout_prob=0.1,\n",
    "                 epsilon=1,\n",
    "                 epsilon_rate=0.99,\n",
    "                 network_input_shape=(2, 84, 84),\n",
    "                 load_path='',\n",
    "                 logger=None):\n",
    "\n",
    "        # Parameters\n",
    "        self.actions = actions  # Size of the discreet action space\n",
    "        self.batch_size = batch_size  # Size of the batch to train the network\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Coefficient for epsilon-greedy exploration\n",
    "        self.epsilon_rate = epsilon_rate  # Rate at which to make epsilon smaller, as training improves the agent's performance; epsilon = epsilon * rate\n",
    "        self.min_epsilon = 0.3  # Minimum epsilon value\n",
    "        # Experience variables\n",
    "        self.experiences = []\n",
    "        self.training_count = 0\n",
    "\n",
    "        # Instantiate the deep Q-network\n",
    "        self.DQN = DQNetwork(\n",
    "            self.actions,\n",
    "            network_input_shape,\n",
    "            alpha=alpha,\n",
    "            gamma=self.gamma,\n",
    "            dropout_prob=dropout_prob,\n",
    "            load_path=load_path,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        if logger is not None:\n",
    "            logger.log({\n",
    "                'Learning rate': alpha,\n",
    "                'Discount factor': self.gamma,\n",
    "                'Starting epsilon': self.epsilon,\n",
    "                'Epsilon decrease rate': self.epsilon_rate,\n",
    "                'Batch size': self.batch_size\n",
    "            })\n",
    "\n",
    "    def get_action(self, state, testing=False):\n",
    "        \"\"\"\n",
    "        Poll DCN for Q-values, return greedy action with probability 1-epsilon\n",
    "        :param state: a state of the MDP with the same size as the DQN input\n",
    "        :param testing: whether to force a greedy action\n",
    "        :return: the selected action\n",
    "        \"\"\"\n",
    "        q_values = self.DQN.predict(state)\n",
    "        if (random.random() < self.epsilon) and not testing:\n",
    "            return random.randint(0, self.actions - 1)\n",
    "        else:\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def add_experience(self, source, action, reward, dest, final):\n",
    "        \"\"\"\n",
    "        Add a tuple (source, action, reward, dest, final) to experiences.\n",
    "        :param source: a state of the MDP\n",
    "        :param action: the action associated to the transition\n",
    "        :param reward: the reward associated to the transition\n",
    "        :param dest: a state of the MDP\n",
    "        :param final: whether the destination state is an absorbing state\n",
    "        \"\"\"\n",
    "        self.experiences.append({'source': source,\n",
    "                                 'action': action,\n",
    "                                 'reward': reward,\n",
    "                                 'dest': dest,\n",
    "                                 'final': final})\n",
    "\n",
    "    def sample_batch(self):\n",
    "        \"\"\"\n",
    "        Pops self.batch_size random samples from experiences and return them as\n",
    "        a batch.\n",
    "        \"\"\"\n",
    "        out = [self.experiences.pop(random.randrange(0, len(self.experiences)))\n",
    "               for _ in range(self.batch_size)]\n",
    "        return np.asarray(out)\n",
    "\n",
    "    def must_train(self):\n",
    "        \"\"\"\"\n",
    "        Returns true if the number of samples in experiences is greater than the\n",
    "        batch size.\n",
    "        \"\"\"\n",
    "        return len(self.experiences) >= self.batch_size\n",
    "\n",
    "    def train(self, update_epsilon=True):\n",
    "        \"\"\"\n",
    "        Samples a batch from experiences, trains the DQN on it, and updates the\n",
    "        epsilon-greedy coefficient.\n",
    "        \"\"\"\n",
    "        self.training_count += 1\n",
    "        print ('Training session #', self.training_count, ' - epsilon:', self.epsilon)\n",
    "        batch = self.sample_batch()\n",
    "        self.DQN.train(batch)  # Train the DQN\n",
    "        if update_epsilon:\n",
    "            self.epsilon = self.epsilon * self.epsilon_rate if self.epsilon > self.min_epsilon else self.min_epsilon  # Decrease the probability of picking a random action to improve exploitation\n",
    "\n",
    "    def quit(self):\n",
    "        \"\"\"\n",
    "        Saves the DQN to disk.\n",
    "        \"\"\"\n",
    "        self.DQN.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pygame.locals import *\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import pygame\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "MAX_EPISODE_LENGTH_FACTOR = 200\n",
    "MAX_EPISODES_BETWEEN_TRAININGS = 1500\n",
    "ACTIONS = 4\n",
    "SCREENSHOT_DIMS = (84, 84)\n",
    "APPLE_REWARD = 1\n",
    "DEATH_REWARD = -1\n",
    "LIFE_REWARD = 0\n",
    "\n",
    "# GAME CONSTANTS\n",
    "pygame.init()\n",
    "\n",
    "display_width = 300\n",
    "display_height = 300\n",
    "\n",
    "fps = 30\n",
    "white = (255,255,255)\n",
    "black = (0,0,0)\n",
    "red = (255,0,0)\n",
    "green = (0,155,0)\n",
    "\n",
    "# direction = \"right\"\n",
    "dirs = [\"left\", \"right\", \"up\", \"down\"]\n",
    "\n",
    "icon = pygame.image.load('Assets/appleImg.png')\n",
    "img = pygame.image.load('Assets/snakehead.png')\n",
    "appleimg = pygame.image.load('Assets/apple.png')\n",
    "\n",
    "gameDisplay = pygame.display.set_mode((display_width,display_height))\n",
    "pygame.display.set_caption('Snake')\n",
    "pygame.display.set_icon(icon)\n",
    "\n",
    "block_size = 10\n",
    "AppleThickness = 20\n",
    "snake_speed = 5\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "smallfont = pygame.font.SysFont('Trebuchet MS', 14)\n",
    "medfont = pygame.font.SysFont('Trebuchet MS', 24)\n",
    "largefont = pygame.font.SysFont('Trebuchet MS', 52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGUMENTS\n",
    "args_train = True\n",
    "args_load = ''\n",
    "args_iterations = -1\n",
    "args_gamma = 0.95\n",
    "args_dropout = 0.1\n",
    "\n",
    "remaining_iters = args_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_game():\n",
    "    global lead_x, lead_y, direction, randAppleX, randAppleY, snakeLength, snakeList, \\\n",
    "    episode_length, episode_reward, gameDisplay, action, state, next_state, must_die\n",
    "    \n",
    "    lead_x = display_width/2\n",
    "    lead_y = display_height/2\n",
    "    direction = random.choice(dirs)\n",
    "    snakeList = []\n",
    "    snakeLength = 1 # will be used for score as (snakeLength-1)\n",
    "    episode_length = 0\n",
    "    episode_reward = 0\n",
    "    must_die = False\n",
    "    randAppleX, randAppleY = randAppleGen()\n",
    "    \n",
    "    # The direction is randomly selected\n",
    "    action = random.randint(0, ACTIONS - 1)\n",
    "    # Initialize the states\n",
    "    state = [screenshot(), screenshot()]\n",
    "    next_state = [screenshot(), screenshot()]\n",
    "    \n",
    "    gameDisplay.fill(white)\n",
    "    gameDisplay.blit(appleimg, (randAppleX, randAppleY))\n",
    "    \n",
    "    snakeHead = []\n",
    "    snakeHead.append(lead_x)\n",
    "    snakeHead.append(lead_y)\n",
    "    snakeList.append(snakeHead)\n",
    "\n",
    "    snake(block_size, snakeList)\n",
    "    Score(snakeLength-1)\n",
    "    pygame.display.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snake(block_size, snakeList):\n",
    "    if direction == \"right\":\n",
    "        head = pygame.transform.rotate(img, 270)\n",
    "    if direction == \"left\":\n",
    "        head = pygame.transform.rotate(img, 90)\n",
    "    if direction == \"up\":\n",
    "        head = pygame.transform.rotate(img, 0)\n",
    "    if direction == \"down\":\n",
    "        head = pygame.transform.rotate(img, 180)\n",
    "\n",
    "    gameDisplay.blit(head, (snakeList[-1][0], snakeList[-1][1]))\n",
    "    for XnY in snakeList[:-1]:\n",
    "        pygame.draw.rect(gameDisplay, green, [XnY[0],XnY[1],block_size,block_size])\n",
    "\n",
    "def randAppleGen():\n",
    "    randAppleX = round(random.randrange(0, display_width - AppleThickness) / 10.0) * 10.0\n",
    "    randAppleY = round(random.randrange(0, display_height - AppleThickness) / 10.0) * 10.0\n",
    "\n",
    "    return randAppleX, randAppleY\n",
    "\n",
    "def pause():\n",
    "    paused = True\n",
    "\n",
    "    while paused:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_SPACE:\n",
    "                    paused = False\n",
    "                elif event.key == pygame.K_q:\n",
    "                    pygame.quit()\n",
    "                    quit()\n",
    "        gameDisplay.fill(white)\n",
    "        message_to_screen(\"Paused\", black, -100, 'large')\n",
    "        message_to_screen(\"Press 'Space' to continue or 'Q' to quit.\", black, 25)\n",
    "\n",
    "        pygame.display.update()\n",
    "        clock.tick(5)\n",
    "\n",
    "def Score(score):\n",
    "    text = smallfont.render(\"Score: \"+str(score), True, black)\n",
    "    gameDisplay.blit(text, [0,0])\n",
    "\n",
    "def text_objects(text, color, size):\n",
    "    if size == \"small\":\n",
    "        textSurface = smallfont.render(text, True, color)\n",
    "    if size == \"medium\":\n",
    "        textSurface = medfont.render(text, True, color)\n",
    "    if size == \"large\":\n",
    "        textSurface = largefont.render(text, True, color)\n",
    "    return textSurface, textSurface.get_rect()\n",
    "    \n",
    "def message_to_screen(msg, color, y_displace=0, size = \"small\"):\n",
    "    textSurf, textRect = text_objects(msg, color, size)\n",
    "    textRect.center = (display_width / 2), (display_height / 2)+y_displace\n",
    "    gameDisplay.blit(textSurf, textRect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def screenshot():\n",
    "    \"\"\"\n",
    "    Takes a screenshot of the game, converts it to greyscale, resizes it to\n",
    "    60x60 and returns it as np.array\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global gameDisplay, is_headless\n",
    "    data = pygame.image.tostring(gameDisplay, 'RGB')  # Take screenshot\n",
    "    image = Image.frombytes('RGB', (display_width, display_height), data)\n",
    "    image = image.convert('L')  # Convert to greyscale\n",
    "    image = image.resize(SCREENSHOT_DIMS)  # Resize\n",
    "#     image = ImageOps.invert(image) if is_headless else image  # TODO ???\n",
    "    image = image.convert('1')\n",
    "    matrix = np.asarray(image.getdata(), dtype=np.float64)\n",
    "    return matrix.reshape(image.size[0], image.size[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def die():\n",
    "    global logger, remaining_iters, snakeLength, episode_length, episode_reward, \\\n",
    "        must_test, experience_buffer, exp_backup_counter, global_episode_counter\n",
    "    \n",
    "    global_episode_counter += 1\n",
    "    # If agent is stuck, kill the process\n",
    "    if global_episode_counter > MAX_EPISODES_BETWEEN_TRAININGS:\n",
    "        print('Shutting process down because something seems to have gone '\n",
    "                   'wrong during training. Please manually check that '\n",
    "                   'all is OK and restart the training.')\n",
    "        DQA.quit()\n",
    "        sys.exit(0)\n",
    "        \n",
    "    must_test = False\n",
    "    \n",
    "    # Add the episode to the experience buffer\n",
    "    if snakeLength-1 >= 1 and episode_length >= 10:\n",
    "        exp_backup_counter += len(experience_buffer)\n",
    "        print ('Adding episode to experiences - Score: %s; Episode length: %s' \\\n",
    "              % (snakeLength-1, episode_length))\n",
    "        print ('Got %s samples of %s' % (exp_backup_counter, DQA.batch_size))\n",
    "        for exp in experience_buffer:\n",
    "            DQA.add_experience(*exp)\n",
    "            \n",
    "    # Train the network (need to replace args.train)\n",
    "    if DQA.must_train() and args_train:\n",
    "        exp_backup_counter = 0\n",
    "        global_episode_counter = 0\n",
    "        # Quit at the last iteration\n",
    "        if remaining_iters == 0:\n",
    "            DQA.quit()\n",
    "            sys.exit(0)\n",
    "\n",
    "        # Train the DQN\n",
    "        DQA.train()\n",
    "\n",
    "        remaining_iters -= 1 if remaining_iters != -1 else 0\n",
    "        # After training, the next episode will be a test one\n",
    "        must_test = True\n",
    "\n",
    "    experience_buffer = []\n",
    "\n",
    "    # Update graphics and restart episode\n",
    "    pygame.display.update()\n",
    "    init_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahil\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (2, 2), data_format=\"channels_first\", padding=\"valid\", strides=(2, 2))`\n",
      "C:\\Users\\sahil\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (2, 2), data_format=\"channels_first\", padding=\"valid\", strides=(2, 2))`\n",
      "C:\\Users\\sahil\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), data_format=\"channels_first\", padding=\"valid\", strides=(2, 2))`\n"
     ]
    }
   ],
   "source": [
    "# Agent\n",
    "DQA = DQAgent(\n",
    "    ACTIONS,\n",
    "    gamma=args_gamma,\n",
    "    dropout_prob=args_dropout,\n",
    "    load_path=args_load,\n",
    "    logger=None\n",
    ")\n",
    "experience_buffer = []  # This will store the SARS tuples at each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats\n",
    "snakeLength = -1 # equivalent to score = 0\n",
    "episode_length = 0\n",
    "episode_reward = 0\n",
    "episode_nb = 0\n",
    "exp_backup_counter = 0\n",
    "global_episode_counter = 0  # Keeps track of how many episodes there were between traning iterations\n",
    "must_test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the game variables\n",
    "lead_x = display_width/2\n",
    "lead_y = display_height/2\n",
    "direction = random.choice(dirs)\n",
    "snakeList = []\n",
    "snakeLength = 1 # will be used for score as (snakeLength-1)\n",
    "episode_length = 0\n",
    "episode_reward = 0\n",
    "must_die = False\n",
    "randAppleX, randAppleY = randAppleGen()\n",
    "\n",
    "# The direction is randomly selected\n",
    "action = random.randint(0, ACTIONS - 1)\n",
    "# Initialize the states\n",
    "state = [screenshot(), screenshot()]\n",
    "next_state = [screenshot(), screenshot()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding episode to experiences - Score: 1; Episode length: 851\n",
      "Got 1410 samples of 1024\n",
      "Training session # 1  - epsilon: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahil\\Anaconda3\\lib\\site-packages\\keras\\models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1024/1024 [==============================] - 32s 31ms/step - loss: 164.7056 - acc: 0.8291\n",
      "Adding episode to experiences - Score: 1; Episode length: 394\n",
      "Got 394 samples of 1024\n",
      "Adding episode to experiences - Score: 1; Episode length: 360\n",
      "Got 754 samples of 1024\n",
      "Training session # 2  - epsilon: 0.99\n",
      "Epoch 1/1\n",
      "1024/1024 [==============================] - 32s 31ms/step - loss: 82.6090 - acc: 0.8213\n",
      "Adding episode to experiences - Score: 2; Episode length: 543\n",
      "Got 543 samples of 1024\n",
      "Adding episode to experiences - Score: 1; Episode length: 495\n",
      "Got 1038 samples of 1024\n",
      "Training session # 3  - epsilon: 0.9801\n",
      "Epoch 1/1\n",
      "1024/1024 [==============================] - 33s 32ms/step - loss: 41.0005 - acc: 0.7158\n",
      "Adding episode to experiences - Score: 1; Episode length: 203\n",
      "Got 203 samples of 1024\n",
      "Adding episode to experiences - Score: 2; Episode length: 311\n",
      "Got 514 samples of 1024\n",
      "Adding episode to experiences - Score: 2; Episode length: 669\n",
      "Got 1183 samples of 1024\n",
      "Training session # 4  - epsilon: 0.9702989999999999\n",
      "Epoch 1/1\n",
      "1024/1024 [==============================] - 30s 30ms/step - loss: 20.6396 - acc: 0.7227\n",
      "Adding episode to experiences - Score: 1; Episode length: 239\n",
      "Got 239 samples of 1024\n",
      "Adding episode to experiences - Score: 1; Episode length: 492\n",
      "Got 731 samples of 1024\n",
      "Adding episode to experiences - Score: 1; Episode length: 391\n",
      "Got 1122 samples of 1024\n",
      "Training session # 5  - epsilon: 0.96059601\n",
      "Epoch 1/1\n",
      "1024/1024 [==============================] - 31s 30ms/step - loss: 15.5354 - acc: 0.7207\n",
      "Adding episode to experiences - Score: 1; Episode length: 276\n",
      "Got 276 samples of 1024\n",
      "Adding episode to experiences - Score: 3; Episode length: 182\n",
      "Got 458 samples of 1024\n",
      "Adding episode to experiences - Score: 1; Episode length: 556\n",
      "Got 1014 samples of 1024\n",
      "Training session # 6  - epsilon: 0.9509900498999999\n",
      "Epoch 1/1\n",
      "1024/1024 [==============================] - 30s 30ms/step - loss: 12.6678 - acc: 0.7900\n",
      "Adding episode to experiences - Score: 1; Episode length: 148\n",
      "Got 148 samples of 1024\n",
      "Adding episode to experiences - Score: 1; Episode length: 374\n",
      "Got 522 samples of 1024\n",
      "Adding episode to experiences - Score: 1; Episode length: 340\n",
      "Got 862 samples of 1024\n",
      "Training session # 7  - epsilon: 0.9414801494009999\n",
      "Epoch 1/1\n",
      "1024/1024 [==============================] - 30s 30ms/step - loss: 10.9394 - acc: 0.8242\n",
      "Adding episode to experiences - Score: 1; Episode length: 147\n",
      "Got 147 samples of 1024\n",
      "Adding episode to experiences - Score: 1; Episode length: 701\n",
      "Got 848 samples of 1024\n",
      "Training session # 8  - epsilon: 0.9320653479069899\n",
      "Epoch 1/1\n",
      "1024/1024 [==============================] - 30s 30ms/step - loss: 7.4300 - acc: 0.7051\n",
      "Adding episode to experiences - Score: 1; Episode length: 173\n",
      "Got 173 samples of 1024\n",
      "Adding episode to experiences - Score: 2; Episode length: 447\n",
      "Got 620 samples of 1024\n",
      "Adding episode to experiences - Score: 1; Episode length: 332\n",
      "Got 952 samples of 1024\n",
      "Adding episode to experiences - Score: 1; Episode length: 85\n",
      "Got 1037 samples of 1024\n",
      "Training session # 9  - epsilon: 0.92274469442792\n",
      "Epoch 1/1\n",
      "1024/1024 [==============================] - 30s 30ms/step - loss: 5.1487 - acc: 0.7139\n",
      "Adding episode to experiences - Score: 1; Episode length: 297\n",
      "Got 297 samples of 1024\n",
      "Adding episode to experiences - Score: 1; Episode length: 344\n",
      "Got 641 samples of 1024\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-dfe80ce13a16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;31m# Poll the DQAgent to get the next action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmust_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;31m# Stopping condition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-00fdc03bb530>\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(self, state, testing)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mselected\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \"\"\"\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtesting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-350ff59dae7a>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \"\"\"\n\u001b[0;32m     97\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[1;32m-> 1064\u001b[1;33m                                   steps=steps)\n\u001b[0m\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1833\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1834\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[1;32m-> 1835\u001b[1;33m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[0;32m   1836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1837\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[1;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1329\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1331\u001b[1;33m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gameExit = False\n",
    "while not gameExit:\n",
    "    episode_length += 1\n",
    "    reward = LIFE_REWARD\n",
    "    next_state[0] = state[1]\n",
    "\n",
    "    # Execute game tick and poll for system events\n",
    "    clock.tick(fps)\n",
    "    for e in pygame.event.get():\n",
    "        if e.type == QUIT:\n",
    "            DQA.quit()\n",
    "            sys.exit(0)\n",
    "\n",
    "    # Change direction according to the action\n",
    "    if action == 0 and direction != \"right\":\n",
    "        direction = \"left\"\n",
    "        lead_x_change = -snake_speed\n",
    "        lead_y_change = 0\n",
    "    elif action == 1 and direction != \"left\":\n",
    "        direction = \"right\"\n",
    "        lead_x_change = snake_speed\n",
    "        lead_y_change = 0\n",
    "    elif action == 2 and direction != \"down\":\n",
    "        direction = \"up\"\n",
    "        lead_y_change = -snake_speed\n",
    "        lead_x_change = 0\n",
    "    elif action == 3 and direction != \"up\":\n",
    "        direction = \"down\"\n",
    "        lead_y_change = snake_speed\n",
    "        lead_x_change = 0 \n",
    "        \n",
    "    # Eats apple\n",
    "    if lead_x > randAppleX and lead_x < randAppleX + AppleThickness or lead_x + block_size > randAppleX and lead_x + block_size < randAppleX + AppleThickness:\n",
    "        if lead_y > randAppleY and lead_y < randAppleY + AppleThickness:\n",
    "            randAppleX, randAppleY = randAppleGen()\n",
    "            snakeLength += 1\n",
    "            reward = APPLE_REWARD\n",
    "        elif lead_y + block_size > randAppleY and lead_y + block_size < randAppleY + AppleThickness:\n",
    "            randAppleX, randAppleY = randAppleGen()\n",
    "            snakeLength += 1\n",
    "            reward = APPLE_REWARD\n",
    "            \n",
    "    # Hits itself\n",
    "    for eachSegment in snakeList[:-1]:\n",
    "        if eachSegment == snakeHead:\n",
    "            must_die = True\n",
    "            reward = DEATH_REWARD\n",
    "            \n",
    "    # Hits walls\n",
    "    if lead_x >= display_width - block_size or lead_x < 0 or lead_y >= display_height - block_size or lead_y < 0:\n",
    "        must_die = True\n",
    "        reward = DEATH_REWARD\n",
    "        \n",
    "    # Now move the snake and update the game screen\n",
    "    lead_x += lead_x_change\n",
    "    lead_y += lead_y_change\n",
    "\n",
    "    gameDisplay.fill(white)\n",
    "    gameDisplay.blit(appleimg, (randAppleX, randAppleY))\n",
    "\n",
    "    snakeHead = []\n",
    "    snakeHead.append(lead_x)\n",
    "    snakeHead.append(lead_y)\n",
    "    snakeList.append(snakeHead)\n",
    "\n",
    "    if len(snakeList) > snakeLength:\n",
    "        del snakeList[0]\n",
    "\n",
    "    snake(block_size, snakeList)\n",
    "    Score(snakeLength-1)\n",
    "    pygame.display.update()\n",
    "    \n",
    "    # Update next state\n",
    "    next_state[1] = screenshot()\n",
    "\n",
    "    # Add SARS tuple to experience_buffer\n",
    "    experience_buffer.append((np.asarray([state]), action, reward,\n",
    "                              np.asarray([next_state]),\n",
    "                              True if must_die else False))\n",
    "    episode_reward += reward\n",
    "\n",
    "    # Change current state\n",
    "    state = list(next_state)\n",
    "\n",
    "    # Poll the DQAgent to get the next action\n",
    "    action = DQA.get_action(np.asarray([state]), testing=must_test)\n",
    "\n",
    "    # Stopping condition\n",
    "    if must_die or episode_length > len(snakeList) * MAX_EPISODE_LENGTH_FACTOR: #len(snakeList is experimental)\n",
    "        die()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] [Playing Snake with Deep Q-Learning by Daniele Grattarola](https://github.com/danielegrattarola/deep-q-snake)\n",
    "\n",
    "[2] [Pygame (Python Game Development)](https://www.youtube.com/playlist?list=PL6gx4Cwl9DGAjkwJocj7vlc_mFU-4wXJq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

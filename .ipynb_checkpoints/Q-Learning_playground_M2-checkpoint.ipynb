{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q(s,a) = r + γ(max(Q(s’,a’))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This says that the Q-value for a given state (s) and action (a) should represent the current reward (r) plus the maximum discounted (γ) future reward expected according to our own table for the next state (s’) we would end up in. The discount variable allows us to decide how important the possible future rewards are compared to the present reward. By updating in this way, the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)\n",
    "#     env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.4895\n"
     ]
    }
   ],
   "source": [
    "print (\"Score over time: \" +  str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Table Values\n",
      "[[8.17798474e-02 3.57501328e-03 3.67419579e-03 6.78326790e-03]\n",
      " [7.22838775e-04 1.66094851e-04 7.26340854e-04 9.20627993e-02]\n",
      " [5.25946114e-03 4.92073634e-03 5.62991342e-03 6.15117454e-02]\n",
      " [7.99329770e-06 1.13712217e-03 4.95138091e-04 3.60054616e-02]\n",
      " [3.74942701e-02 3.84489581e-06 2.79617155e-04 3.87779385e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.17154627e-01 1.60833577e-04 2.12444102e-04 6.01938728e-07]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.51181519e-03 5.39413709e-04 2.94383332e-03 1.54227208e-02]\n",
      " [0.00000000e+00 4.34718246e-02 7.39328000e-04 0.00000000e+00]\n",
      " [1.18890338e-01 6.44022296e-04 1.03772155e-03 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.31258580e-03 0.00000000e+00 1.53647518e-01 1.21507794e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 8.28420531e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Final Q-Table Values\")\n",
    "print (Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain Car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning example using OpenAI gym MountainCar enviornment\n",
    "\n",
    "Author: Moustafa Alzantot (malzantot@ucla.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 40\n",
    "iter_max = 10000\n",
    "\n",
    "initial_lr = 1.0 # Learning rate\n",
    "min_lr = 0.003\n",
    "gamma = 1.0\n",
    "t_max = 10000\n",
    "eps = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy=None, render=False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    for _ in range(t_max):\n",
    "        if render:\n",
    "            env.render()\n",
    "        if policy is None:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            a,b = obs_to_state(env, obs)\n",
    "            action = policy[a][b]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += gamma ** step_idx * reward\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_to_state(env, obs):\n",
    "    \"\"\" Maps an observation to state \"\"\"\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    env_dx = (env_high - env_low) / n_states\n",
    "    a = int((obs[0] - env_low[0])/env_dx[0])\n",
    "    b = int((obs[1] - env_low[1])/env_dx[1])\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score of solution =  -129.96\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env_name = 'MountainCar-v0'\n",
    "    env = gym.make(env_name)\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "    print ('----- using Q Learning -----')\n",
    "    q_table = np.zeros((n_states, n_states, 3))\n",
    "    for i in range(iter_max):\n",
    "        obs = env.reset()\n",
    "        total_reward = 0\n",
    "        ## eta: learning rate is decreased at each step\n",
    "        eta = max(min_lr, initial_lr * (0.85 ** (i//100)))\n",
    "        for j in range(t_max):\n",
    "            a, b = obs_to_state(env, obs)\n",
    "            if np.random.uniform(0, 1) < eps:\n",
    "                action = np.random.choice(env.action_space.n)\n",
    "            else:\n",
    "                logits = q_table[a][b]\n",
    "                logits_exp = np.exp(logits)\n",
    "                probs = logits_exp / np.sum(logits_exp)\n",
    "#                 from IPython.core.debugger import Tracer; Tracer()()\n",
    "                action = np.random.choice(env.action_space.n, p=probs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            # update q table\n",
    "            a_, b_ = obs_to_state(env, obs)\n",
    "            q_table[a][b][action] = q_table[a][b][action] + eta * (reward + gamma *  np.max(q_table[a_][b_]) - q_table[a][b][action])\n",
    "            if done:\n",
    "                break\n",
    "        if i % 100 == 0:\n",
    "            display(print('Iteration #%d -- Total reward = %d.' %(i+1, total_reward)))\n",
    "            clear_output(wait=True)\n",
    "    solution_policy = np.argmax(q_table, axis=2)\n",
    "    solution_policy_scores = [run_episode(env, solution_policy, False) for _ in range(100)]\n",
    "    print(\"Average score of solution = \", np.mean(solution_policy_scores))\n",
    "    \n",
    "    # Animate it\n",
    "    run_episode(env, solution_policy, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with a playing agent using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a neural network that, given the state of the game (actually, two consecutive states), it outputs a family of quality values (Q-values) for each next possible move. The move with higher Q-value is chosen and performed in the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahil\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZATION: libraries, parameters, network...\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "from keras.models import Sequential      # One layer after the other\n",
    "from keras.layers import Dense, Flatten, Dropout  # Dense layers are fully connected layers, Flatten layers flatten out multidimensional inputs\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque            # For storing moves \n",
    "\n",
    "import numpy as np\n",
    "import gym                                # To train our network\n",
    "env = gym.make('MountainCar-v0')          # Choose game (any in the gym should work)\n",
    "\n",
    "import random     # For sampling batches from the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create network. Input is two consecutive game states, output is Q-values of the possible moves.\n",
    "# model = Sequential()\n",
    "# model.add(Dense(256, input_shape=(2,) + env.observation_space.shape, init='uniform', activation='relu'))\n",
    "# model.add(Flatten())       # Flatten input so as to have no problems with processing\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.40))\n",
    "# model.add(Dense(env.action_space.n, activation='softmax'))    # Same number of outputs as possible actions\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer='rmsprop',\n",
    "#                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(24, input_shape=(2,) + env.observation_space.shape, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(env.action_space.n, activation='softmax'))\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "D = deque()                                # Register where the actions will be stored\n",
    "\n",
    "observetime = 5000                         # Number of timesteps we will be acting on the game and observing results\n",
    "epsilon = 0.7                              # Probability of doing a random move\n",
    "gamma = 0.9                                # Discounted future reward. How much we care about steps further in time\n",
    "mb_size = 1                               # Learning minibatch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observing Finished\n",
      "Number of actions captured: 5000\n"
     ]
    }
   ],
   "source": [
    "# FIRST STEP: Knowing what each action does (Observing)\n",
    "\n",
    "observation = env.reset()                     # Game begins\n",
    "obs = np.expand_dims(observation, axis=0)     # (Formatting issues) Making the observation the first element of a batch of inputs \n",
    "state = np.stack((obs, obs), axis=1)\n",
    "done = False\n",
    "for t in range(observetime):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        action = np.random.randint(0, env.action_space.n, size=1)[0]\n",
    "    else:\n",
    "        Q = model.predict(state)          # Q-values predictions\n",
    "        action = np.argmax(Q)             # Move with highest Q-value is the chosen one\n",
    "    observation_new, reward, done, info = env.step(action)     # See state of the game, reward... after performing the action\n",
    "    obs_new = np.expand_dims(observation_new, axis=0)          # (Formatting issues)\n",
    "    state_new = np.append(np.expand_dims(obs_new, axis=0), state[:, :1, :], axis=1)     # Update the input with the new state of the game\n",
    "    D.append((state, action, reward, state_new, done))         # 'Remember' action and consequence\n",
    "    state = state_new         # Update state\n",
    "    if done:\n",
    "        env.reset()           # Restart game if it's finished\n",
    "        obs = np.expand_dims(observation, axis=0)     # (Formatting issues) Making the observation the first element of a batch of inputs \n",
    "        state = np.stack((obs, obs), axis=1)\n",
    "    \n",
    "    #Adding code to observe Q-values\n",
    "#     if t % 10 == 0:\n",
    "#         print(\"Q:\", Q)\n",
    "print('Observing Finished')\n",
    "print('Number of actions captured: {}'.format(len(D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Finished\n"
     ]
    }
   ],
   "source": [
    "# SECOND STEP: Learning from the observations (Experience replay)\n",
    "\n",
    "minibatch = random.sample(D, mb_size)                              # Sample some moves\n",
    "\n",
    "inputs_shape = (mb_size,) + state.shape[1:]\n",
    "inputs = np.zeros(inputs_shape)\n",
    "targets = np.zeros((mb_size, env.action_space.n))\n",
    "\n",
    "for i in range(0, mb_size):\n",
    "    state = minibatch[i][0]\n",
    "    action = minibatch[i][1]\n",
    "    reward = minibatch[i][2]\n",
    "    state_new = minibatch[i][3]\n",
    "    done = minibatch[i][4]\n",
    "    \n",
    "# Build Bellman equation for the Q function\n",
    "    inputs[i:i+1] = np.expand_dims(state, axis=0)\n",
    "    targets[i] = model.predict(state)\n",
    "    Q_sa = model.predict(state_new)\n",
    "    \n",
    "    if done:\n",
    "        targets[i, action] = reward\n",
    "    else:\n",
    "        targets[i, action] = reward + gamma * np.max(Q_sa)\n",
    "\n",
    "# Train network to output the Q function\n",
    "    model.train_on_batch(inputs, targets)\n",
    "#     model.fit(inputs, targets, verbose=1, epochs=1)\n",
    "print('Learning Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (env.spec.id, step, info))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game ended! Total reward: -1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# THIRD STEP: Play!\n",
    "\n",
    "observation = env.reset()\n",
    "obs = np.expand_dims(observation, axis=0)\n",
    "state = np.stack((obs, obs), axis=1)\n",
    "done = False\n",
    "tot_reward = 0.0\n",
    "while not done:\n",
    "#     env.render()                    # Uncomment to see game running\n",
    "    show_state(env)\n",
    "    Q = model.predict(state)        \n",
    "    action = np.argmax(Q)         \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    obs = np.expand_dims(observation, axis=0)\n",
    "    state = np.append(np.expand_dims(obs, axis=0), state[:, :1, :], axis=1)    \n",
    "    tot_reward += reward\n",
    "    \n",
    "print('Game ended! Total reward: {}'.format(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
